A project to revive the LLM responses in [Stewbot](https://github.com/KestronProgramming/Stewbot) by running self-hosted LLM agents on multiple servers with GPUs.

---

For linux, ollama can be istalled with:

```bash
curl -fsSL https://ollama.com/install.sh | sh
```

For windows, ollama can be installed with:
```powershell
winget install Ollama.Ollama
```

All issues cean be blamed on [@Reginald-Gillespie](https://github.com/Reginald-Gillespie)